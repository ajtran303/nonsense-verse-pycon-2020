{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some corpus-driven approaches\n",
    "\n",
    "By [Allison Parrish](http://www.decontextualize.com/)\n",
    "\n",
    "This notebook uses `pronouncing` and Pincelate to \"find\" poems in existing corpora. I show an example of searching prose for Haiku, and an example of re-rhyming lines in an existing corpus of poetry.\n",
    "\n",
    "First, some code preliminaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pronouncing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from pincelate import Pincelate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Pincelate model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pin = Pincelate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function tries to use [pronouncing](https://pronouncing.readthedocs.io/) to look up a word's pronunciation, but if it's not there, it uses [Pincelate](https://pincelate.readthedocs.io/en/latest/) instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_phones(w):\n",
    "    pfw = pronouncing.phones_for_word(w)\n",
    "    if len(pfw) == 0:\n",
    "        # \"normalize\" so the model will work\n",
    "        w = re.sub(\"[^a-z']\", \"\", w)\n",
    "        return \" \".join(pin.soundout(w))\n",
    "    else:\n",
    "        return pfw[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should work even on made-up words that aren't in the CMU pronouncing dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'P IY0 K AA1 CH UW0'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quick_phones(\"pikachu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the function gets a string with any characters that aren't in Pincelate's model, it simply removes them. This isn't an ideal solution, but it at least means we won't get any errors, and sometimes it works okay(ish):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HH AA1 L OW0'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quick_phones(\"h√©llo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "If we're going to work with a corpus, we need to find ways to divide it up into meaningful units. Splitting a text up into meaningful units is called \"tokenization.\" We'll use [NLTK](https://www.nltk.org/)'s `punkt` tokenizer to get sentences and words from the text. If you followed the installation instructions, you should already have NLTK installed, but you'll still need to download the relevant model. You can do this by running the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/allison/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function to split a string up into sentences is `nltk.sent_tokenize()`. The tokenizer is able to deal with many cases where otherwise sentence-ending punctuation doesn't actually end a sentence, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This... is a test.',\n",
       " \"My mother (Mrs. Parrish) said there'd be days like these.\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.sent_tokenize(\"This... is a test. My mother (Mrs. Parrish) said there'd be days like these.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, `nltk.word_tokenize()` function splits a string up into words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It',\n",
       " 'was',\n",
       " 'the',\n",
       " 'best',\n",
       " 'of',\n",
       " 'times',\n",
       " ',',\n",
       " 'it',\n",
       " 'was',\n",
       " 'the',\n",
       " 'worst',\n",
       " 'of',\n",
       " 'times',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(\"It was the best of times, it was the worst of times.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding haiku\n",
    "\n",
    "The computational haiku has [vexed poets and programmers since at least 1967](http://rwet.decontextualize.com/pdfs/morris.pdf) and it is possible that there is little left to be said about the form. By \"haiku\" here I mean haiku as it has been adapted over the past century or more in English literature (see [Haiku in English](https://en.wikipedia.org/wiki/Haiku_in_English)), which is a form that strips out most of the semantic constraints of the original Japanese form in favor of a purely lexical and metrical definition: such a haiku consists of three lines, the first having five syllables, the second seven syllables, and the third five syllables.\n",
    "\n",
    "We're going to write some code that goes through our corpus and finds haiku, in the manner of [Times Haiku](https://en.wikipedia.org/wiki/Haiku_in_English). Specifically, we'll identify sentences in our corpus that have seventeen syllables and can be partitioned into 5/7/5 lines.\n",
    "\n",
    "Counting syllables is easy; what turns out to be a bit tricky is making sure that the words can be divided up into lines. The way we'll calculate this is to first make a list of syllable counts for each word in a sentence, then check to see if that sequence can be partitioned into a haiku. This function does the partitioning part, returning a list of indices where each line should end in the original list, or an empty list if no partition can be found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_haiku(t):\n",
    "    needed = [5, 7, 5]\n",
    "    indices = [0]*len(needed)\n",
    "    idx = 0\n",
    "    if sum(t) != sum(needed):\n",
    "        return []\n",
    "    for i, count in enumerate(needed):\n",
    "        total = 0\n",
    "        # keep moving through words...\n",
    "        while total < count:\n",
    "            total += t[idx]\n",
    "            # update the index with current position\n",
    "            indices[i] = idx\n",
    "            idx += 1\n",
    "        # if we overshot, this doesn't partition cleanly!\n",
    "        if total != count:\n",
    "            return []\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 9, 12]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "haiku = \"\"\"the west wind whispered\n",
    "and touched the eyelids of spring\n",
    "her eyes primroses\"\"\"\n",
    "partition_haiku([pronouncing.syllable_count(quick_phones(w)) for w in haiku.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the function tells us that line breaks should fall after indices 3, 9 and 12. \n",
    "\n",
    "If the syllable counts can't be partitioned into a haiku the function returns an empty list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partition_haiku([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another quick test: generate sequences of random integers and print out the sequences that can be partitioned as haiku:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 2 2 1 2 2 3] [1, 5, 7]\n",
      "[2 2 1 1 2 1 3 1 4] [2, 6, 8]\n",
      "[4 1 4 2 1 4 1] [1, 4, 6]\n",
      "[1 4 4 1 1 1 1 2 2] [1, 5, 8]\n",
      "[1 4 3 4 2 3] [1, 3, 5]\n",
      "[2 2 1 2 4 1 4 1] [2, 5, 7]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    rand_seq = np.random.randint(1, 5, size=(np.random.randint(14)))\n",
    "    linebreaks = partition_haiku(rand_seq)\n",
    "    if len(linebreaks) > 0:\n",
    "        print(rand_seq, linebreaks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Haiku source text\n",
    "\n",
    "Included in this repository is a copy of Mary Shelley's *Frankenstein* (as `84-0.txt`). We'll use this as the sample text below. You could also use a different file, as long as it's in plain text format. [Project Gutenberg](http://www.gutenberg.org/) is a great place to look for these! Just download the file into the same directory as this notebook and replace the filename in the cell below with the filename of the text you selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(\"84-0.txt\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of sentences in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3199"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sifting through the sentences\n",
    "\n",
    "As a first task, we could just check for sentences that have seventeen syllables. To do this, we'll iterate through each sentence, and do the following:\n",
    "\n",
    "* Get the tokens in the sentence that are relevant to syllable counting (i.e., everything that isn't punctuation)\n",
    "* Use `pronouncing`'s `.syllable_count()` function to see how many syllables are in each word\n",
    "* Sum up the syllable counts and print the sentence if it adds up to 17."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this expedition has been the favourite dream of my early years.\n",
      "oh, that some encouraging voice would answer in the affirmative!\n",
      "remember me with affection, should you never hear from me again.\n",
      "as i spoke, a dark gloom spread over my listener‚Äôs countenance.\n",
      "she was not her child, but the daughter of a milanese nobleman.\n",
      "no human being could have passed a happier childhood than myself.\n",
      "and clerval‚Äîcould aught ill entrench on the noble spirit of clerval?\n",
      "this expectation will now be the consolation of your father.\n",
      "she indeed veiled her grief and strove to act the comforter to us all.\n",
      "i requested his advice concerning the books i ought to procure.\n",
      "to examine the causes of life, we must first have recourse to death.\n",
      "‚Äúdearest clerval,‚Äù exclaimed i, ‚Äúhow kind, how very good you are to me.\n",
      "how pleased you would be to remark the improvement of our ernest!\n",
      "poor justine was very ill; but other trials were reserved for her.\n",
      "henry saw this, and had removed all my apparatus from my view.\n",
      "frankenstein is modest; an excellent quality in a young man.\n",
      "i threw the letter on the table, and covered my face with my hands.\n",
      "but i paused when i reflected on the story that i had to tell.\n",
      "these reflections determined me, and i resolved to remain silent.\n",
      "you perhaps will find some means to justify my poor guiltless justine.\n",
      "i will melt the stony hearts of your enemies by my tears and prayers.\n",
      "how sweet is the affection of others to such a wretch as i am!\n",
      "my wanderings were directed towards the valley of chamounix.\n",
      "for some time i sat upon the rock that overlooks the sea of ice.\n",
      "their icy and glittering peaks shone in the sunlight over the clouds.\n",
      "have i not suffered enough, that you seek to increase my misery?\n",
      "how strange, i thought, that the same cause should produce such opposite effects!\n",
      "i quickly collected some branches, but they were wet and would not burn.\n",
      "but i was baffled in every attempt i made for this purpose.\n",
      "‚Äúspring advanced rapidly; the weather became fine and the skies cloudless.\n",
      "the lady was dressed in a dark suit and covered with a thick black veil.\n",
      "‚Äúas night came on, agatha and the arabian retired early.\n",
      "his plans were facilitated by the news which arrived from paris.\n",
      "many things i read surpassed my understanding and experience.\n",
      "it was your journal of the four months that preceded my creation.\n",
      "i dared not think that they would turn them from me with disdain and horror.\n",
      "who can describe their horror and consternation on beholding me?\n",
      "i could have torn him limb from limb, as the lion rends the antelope.\n",
      "but my heart sank within me as with bitter sickness, and i refrained.\n",
      "i trembled violently, apprehending some dreadful misfortune.\n",
      "how often did i imprecate curses on the cause of my being!\n",
      "my companion must be of the same species and have the same defects.\n",
      "in britain only could he further the execution of his plan.\n",
      "‚Äúdo you,‚Äù said i, ‚Äúenjoy yourself, and let this be our rendezvous.\n",
      "thus situated, my only resource was to drive before the wind.\n",
      "i entered the room where the corpse lay and was led up to the coffin.\n",
      "my father tried to awaken in me the feelings of affection.\n",
      "on that night he had determined to consummate his crimes by my death.\n",
      "but until then, i conjure you, do not mention or allude to it.\n",
      "know that, one by one, my friends were snatched away; i was left desolate.\n",
      "i was answered through the stillness of night by a loud and fiendish laugh.\n",
      "i took my passage in the same ship, but he escaped, i know not how.\n",
      "i cannot doubt it, yet i am lost in surprise and admiration.\n",
      "i would reconcile him to life, but he repulses the idea.\n",
      "my unfortunate guest regards me with the tenderest compassion.\n",
      "frankenstein, who was dozing, awoke and asked the cause of the tumult.\n",
      "this was my duty, but there was another still paramount to that.\n",
      "but i journey towards england, and i may there find consolation.\n",
      "it is midnight; the breeze blows fairly, and the watch on deck scarcely stir.\n",
      "i recollected my threat and resolved that it should be accomplished.\n",
      "even now my blood boils at the recollection of this injustice.\n",
      "he was soon borne away by the waves and lost in darkness and distance.\n",
      "unless you have removed all references to project gutenberg:  1.e.1.\n"
     ]
    }
   ],
   "source": [
    "for i, sent in enumerate(sentences):\n",
    "    sent = sent.lower().replace(\"\\n\", \" \")\n",
    "    words = [w for w in nltk.word_tokenize(sent) if w[0].isalpha()]\n",
    "    syll_counts = [pronouncing.syllable_count(quick_phones(w)) for w in words]\n",
    "    if sum(syll_counts) == 17:\n",
    "        print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are quite a few of these! Our definition of the Haiku, however, says that we need lines of 5/7/5 syllables exactly, without breaking words across lines. The code in the following cell builds on the previous cell, attempting to partition the syllable counts for each line into haiku (using `partition_haiku()` above). If a haiku can be formed, it prints it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this expedition\n",
      "has been the favourite dream\n",
      "of my early years\n",
      "\n",
      "\n",
      "remember me with\n",
      "affection should you never\n",
      "hear from me again\n",
      "\n",
      "\n",
      "no human being\n",
      "could have passed a happier\n",
      "childhood than myself\n",
      "\n",
      "\n",
      "and clerval‚Äîcould aught\n",
      "ill entrench on the noble\n",
      "spirit of clerval\n",
      "\n",
      "\n",
      "i requested his\n",
      "advice concerning the books\n",
      "i ought to procure\n",
      "\n",
      "\n",
      "to examine the\n",
      "causes of life we must first\n",
      "have recourse to death\n",
      "\n",
      "\n",
      "how pleased you would be\n",
      "to remark the improvement\n",
      "of our ernest\n",
      "\n",
      "\n",
      "i threw the letter\n",
      "on the table and covered\n",
      "my face with my hands\n",
      "\n",
      "\n",
      "but i paused when i\n",
      "reflected on the story\n",
      "that i had to tell\n",
      "\n",
      "\n",
      "you perhaps will find\n",
      "some means to justify my\n",
      "poor guiltless justine\n",
      "\n",
      "\n",
      "how strange i thought that\n",
      "the same cause should produce such\n",
      "opposite effects\n",
      "\n",
      "\n",
      "but i was baffled\n",
      "in every attempt i\n",
      "made for this purpose\n",
      "\n",
      "\n",
      "the lady was dressed\n",
      "in a dark suit and covered\n",
      "with a thick black veil\n",
      "\n",
      "\n",
      "many things i read\n",
      "surpassed my understanding\n",
      "and experience\n",
      "\n",
      "\n",
      "i dared not think that\n",
      "they would turn them from me with\n",
      "disdain and horror\n",
      "\n",
      "\n",
      "who can describe their\n",
      "horror and consternation\n",
      "on beholding me\n",
      "\n",
      "\n",
      "i could have torn him\n",
      "limb from limb as the lion\n",
      "rends the antelope\n",
      "\n",
      "\n",
      "how often did i\n",
      "imprecate curses on the\n",
      "cause of my being\n",
      "\n",
      "\n",
      "my companion must\n",
      "be of the same species and\n",
      "have the same defects\n",
      "\n",
      "\n",
      "thus situated\n",
      "my only resource was to\n",
      "drive before the wind\n",
      "\n",
      "\n",
      "i entered the room\n",
      "where the corpse lay and was led\n",
      "up to the coffin\n",
      "\n",
      "\n",
      "on that night he had\n",
      "determined to consummate\n",
      "his crimes by my death\n",
      "\n",
      "\n",
      "but until then i\n",
      "conjure you do not mention\n",
      "or allude to it\n",
      "\n",
      "\n",
      "know that one by one\n",
      "my friends were snatched away i\n",
      "was left desolate\n",
      "\n",
      "\n",
      "i was answered through\n",
      "the stillness of night by a\n",
      "loud and fiendish laugh\n",
      "\n",
      "\n",
      "i can not doubt it\n",
      "yet i am lost in surprise\n",
      "and admiration\n",
      "\n",
      "\n",
      "frankenstein who was\n",
      "dozing awoke and asked the\n",
      "cause of the tumult\n",
      "\n",
      "\n",
      "this was my duty\n",
      "but there was another still\n",
      "paramount to that\n",
      "\n",
      "\n",
      "it is midnight the\n",
      "breeze blows fairly and the watch\n",
      "on deck scarcely stir\n",
      "\n",
      "\n",
      "i recollected\n",
      "my threat and resolved that it\n",
      "should be accomplished\n",
      "\n",
      "\n",
      "even now my blood\n",
      "boils at the recollection\n",
      "of this injustice\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, sent in enumerate(sentences):\n",
    "    sent = sent.lower()\n",
    "    # only tokens that look like words\n",
    "    words = [w for w in nltk.word_tokenize(sent) if w[0].isalpha()]\n",
    "    syll_count = 0\n",
    "    try:\n",
    "        syll_counts = [pronouncing.syllable_count(quick_phones(w))\n",
    "                       for w in words]\n",
    "        linebreaks = partition_haiku(syll_counts)\n",
    "        if len(linebreaks) > 0:\n",
    "            # add linebreaks after indices that are in the haiku partition\n",
    "            out = \"\".join([w+(\"\\n\" if i in linebreaks else \" \") for i, w in enumerate(words)])\n",
    "            print(out)\n",
    "            print()\n",
    "    except IndexError: # if any word isn't found, skip\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-rhyming the sonnets\n",
    "\n",
    "This next example serves to demonstrate how to find rhyming lines within a corpus. This time, we'll be using Shakespeare's sonnets as a source text. A copy of the sonnets (with numbering stripped) is included in this repository as `sonnets.txt`. The line below reads these lines into a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonnet_lines = [item for item in open(\"sonnets.txt\").read().split(\"\\n\") if len(item) > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the first few lines, just to make sure the data looks like what we want it to look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['From fairest creatures we desire increase,',\n",
       " \"That thereby beauty's rose might never die,\",\n",
       " 'But as the riper should by time decease,',\n",
       " 'His tender heir might bear his memory:',\n",
       " 'But thou contracted to thine own bright eyes,',\n",
       " \"Feed'st thy light's flame with self-substantial fuel,\",\n",
       " 'Making a famine where abundance lies,',\n",
       " 'Thy self thy foe, to thy sweet self too cruel:',\n",
       " \"Thou that art now the world's fresh ornament,\",\n",
       " 'And only herald to the gaudy spring,']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sonnet_lines[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to determine if two words rhyme, we need to check if their \"rhyming parts\" match. The `pronouncing` library has a functiion `.rhyming_part()` that returns the substring of phones from a word that can be used to determine whether it rhymes with another word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EH1 T IH0 K AH0 L'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pronouncing.rhyming_part(quick_phones(\"alphabetical\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our task is to find *random rhyming couplets* from the sonnets. As a first approximation of accomplishing this task, we'll do the following:\n",
    "\n",
    "* Pick a line at random.\n",
    "* Get that line's rhyming part.\n",
    "* Check the rhyming parts of every other line to see if they match.\n",
    "\n",
    "The code in the cell below implements this. I've interspersed with comments to explain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But then begins a journey in my head\n",
      "Sweet love, renew thy force; be it not said\n"
     ]
    }
   ],
   "source": [
    "picked = random.choice(sonnet_lines)\n",
    "picked_words = [w for w in nltk.word_tokenize(picked) if w[0].isalpha()]\n",
    "# get the rhyming part from the final word in the line\n",
    "picked_rhyme = pronouncing.rhyming_part(quick_phones(picked_words[-1]))\n",
    "\n",
    "# get a shuffled copy of the sonnets line and search until we find a\n",
    "# line that rhymes\n",
    "for line in random.sample(sonnet_lines, len(sonnet_lines)):\n",
    "    # a line \"technically\" rhymes with itself but this is boring\n",
    "    if picked == line:\n",
    "        continue\n",
    "    words = [w for w in nltk.word_tokenize(line) if w[0].isalpha()]\n",
    "    # skip empty lines\n",
    "    if len(words) == 0:\n",
    "        continue\n",
    "    # if this line's rhyming part matches, print this line and the\n",
    "    # picked line\n",
    "    if pronouncing.rhyming_part(quick_phones(words[-1])) == picked_rhyme:\n",
    "        print(picked)\n",
    "        print(line)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This solution works fine, but it's a little slow, since it has to check every other line to see if there's a match! (This is an $O(n^2)$ algorithm.) You might not notice how much time it takes to find a rhyming couplet in a small corpus like the sonnets, but you *will* notice if you're creating thousands of couplets, or if you adapt this code to use on a larger corpus.\n",
    "\n",
    "A better way to do this would be to build a data structure in one pass that contains rhyming parts as keys, and lists of rhyming lines as values. With a data structure like this, you can generate a couplet by selecting a rhyming part at random, then drawing random rhyming lines from the list of lines corresponding to that rhyming part. The code in the cell below implements this solution, with a few additional features:\n",
    "\n",
    "* To prevent rhyming lines that end with the same word, I store (for each rhyming part) a dictionary that maps line-ending words to the lines (with the corresponding rhyming part) that contain them.\n",
    "* I extract the rhyming part not just from the last word, but from the concatenated phones of the last three words in the line. This makes it possible to find rhymes that extend over multiple words.\n",
    "* I use the `collections` package's `defaultdict` as a shortcut for initializing empty nested dictionaries and lists. The data structure we end up is a dictionary whose values are dictionaries whose values are lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "# parameter to defaultdict has to be callable, hence the lambda\n",
    "rhyming_part_to_idx = defaultdict(lambda: defaultdict(list))\n",
    "for i, line in enumerate(sonnet_lines):\n",
    "    words = [w for w in nltk.word_tokenize(line) if w[0].isalpha()]\n",
    "    if len(words) > 0:\n",
    "        last_few = \" \".join([quick_phones(w) for w in words[-3:]])\n",
    "        rhyming_part = pronouncing.rhyming_part(last_few)\n",
    "        # to save space, store only the index of the line, not\n",
    "        # the line itself!\n",
    "        rhyming_part_to_idx[rhyming_part][words[-1]].append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've run this cell, you'll have the data structure! You can see that the keys of the dictionary are all of the unique rhyming parts that occur at the ends of lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['IY1 S', 'AY1', 'EH1 M ER0 IY0', 'AY1 Z', 'UW1 AH0 L', 'AO1 R N AH0 M AH0 N T', 'IH1 NG', 'AA1 N T EH0 N T', 'AA1 R D IH0 NG', 'IY1', 'AW1', 'IY1 L D', 'EH1 L D', 'EY1 Z', 'UW1 S', 'AY1 N', 'OW1 L D', 'UW1 AH0 S T', 'AH1 DH ER0', 'IY1 N UW0', 'UW1 M', 'AH1 Z B AH0 N D R IY0', 'EH1 R AH0 T IY0', 'AY1 M', 'EH1 N D', 'EH1 G AH0 S IY0', 'IH1 V', 'AY1 V', 'OW1 N', 'IY1 V', 'AO1 N', 'EY1 M', 'EH1 L', 'AA1 N', 'EH1 R', 'EH1 F T', 'AE1 S', 'AA1 Z', 'IY1 T', 'EY1 S', 'IH1 L', 'UW1 ZH ER0 IY0', 'AH1 N', 'AA1 R T', 'AY1 T', 'AE1 JH AH0 S T IY0', 'EY1 JH', 'IH1 L G R AH0 M AH0 JH', 'AA1 R', 'EY1', 'UW1 N', 'AE1 D L IY0', 'OY1', 'AW1 N D Z', 'IY1 R', 'AO1 R D ER0 IH0 NG', 'AY1 F', 'IY1 P', 'AY1 N D', 'IH1 T', 'IH1 T S', 'EH1 N IY0', 'AH1 N P R AH0 V AH0 D AH0 N T', 'EH1 V AH0 D AH0 N T', 'EY1 T', 'AY1 ER0', 'AH1 V', 'UW1 V', 'OW1 S T', 'AA1 R T AH0 S T', 'OW1 Z', 'EH1 S T', 'AO1 R', 'EH1 R IH0 SH', 'IY1 V Z', 'ER1 D', 'IH1 R D', 'EY1 K', 'OW1', 'EH1 N S', 'ER1', 'AH1 K', 'AA1 N AH0 M IY0', 'AA1 L AH0 T IY0', 'AA1 N V ER0 T', 'EY2 T', 'OW1 M AH0 N T', 'AA1 M EH0 N T', 'UW1', 'AW1 ER0 Z', 'EH1 T', 'IH2 T', 'EH1 N', 'AH1 M', 'EH1 Z ER0 T S', 'AA1 R T S', 'EY1 S IH0 Z', 'EY1 S AH0 Z', 'AH1 NG', 'AO1 NG', 'EH1 M P R AH0 T', 'AY1 N Z', 'IH1 M', 'EY1 D', 'AO1 Z', 'UW1 D', 'AH1 D', 'IY1 T S', 'EY1 N T IH0 D', 'AE1 SH AH0 N', 'OW1 L IH0 NG', 'AE1 Z IH0 TH', 'EH1 TH', 'EY1 T AH0 D', 'OW1 T IH0 NG', 'IY1 T AH0 D', 'AH1 TH IH0 NG', 'EH1 ZH ER0', 'UW1 Z', 'ER1 S', 'EH1 M Z', 'EH1 R IY0', 'AA1 R IY0', 'EY1 N', 'EH1 L AH0 K W AH0 N S', 'EH2 N S', 'EH1 S', 'AA1 R Z', 'EH1 D', 'EH1 R IY0 D', 'OY1 L', 'EH1 L AA0 V', 'AA1 V', 'AE1 JH', 'AE1 S AH0 JH', 'UW1 V IH0 NG', 'EH2 K T', 'AH1 V IH0 NG', 'EH1 K T', 'AY1 R D', 'AY1 D', 'EH1 V AH0 N', 'IY1 V IH0 N', 'AO1 NG G ER0', 'AO1 NG ER0', 'OW1 P', 'IY1 S T', 'AY1 Z IH0 NG', 'IH1 NG Z', 'AO1 T', 'AE1 S T', 'EY1 S T', 'OW1 ER0', 'IH1 R', 'AH1 V ER0', 'ER1 V IY0', 'IH1 JH', 'IY1 N', 'AE1 L K AH0 M IY0', 'EY1 N IH0 TH', 'OW1 K', 'IY1 K', 'IY1 F', 'AO1 S', 'EH1 D Z', 'IY1 D Z', 'IH1 S', 'AE1 D V AH0 K AH0 T', 'AO1 R T', 'UW1 TH', 'EH1 S P IH0 S', 'AH1 F IH0 K', 'EH1 N T', 'EH1 K S AH0 L AH0 N T', 'ER1 TH', 'AO1 R TH', 'AO1 L', 'IY1 V AH0 S T', 'UW1 S AH0 S T', 'AA1 V ER0 T IY0', 'IH1 N JH ER0 IY0', 'EY1 L', 'IH1 R L IY0', 'EH1 K T IH0 D', 'EH1 K T AH0 D', 'AE1 N D', 'AA2 L IY0', 'AE1 D', 'AE1 N AH0 L D', 'ER1 M AH0 N D', 'UH1 K', 'AH1 S T', 'IY1 F EH0 K T S', 'EH1 K T S', 'AE1 V AH0 T IY0', 'EH1 Z ER0 T', 'UW1 P R IY0 R', 'AH1 F AH0 N S', 'IY1 D', 'IH1 L F AH0 L S OW0', 'EH1 SH AH0 L M AH0 S T', 'IY1 M', 'OW1 Z IH0 Z', 'AO1 N T AH0 N L IY0', 'AA1 N Y AH0 M AH0 N T S', 'AA1 N T EH0 N T S', 'ER2 N', 'EY1 S AH0 N R IY0', 'ER1 N', 'EH1 N M AH0 T IY0', 'AY2 T', 'AH1 L N AH0 S', 'AW1 ER0', 'EY1 V', 'EH1 K', 'IY2', 'IH1 Z', 'AH1 L', 'AY1 L D', 'AW1 N', 'AW1 N D', 'OW1 P AH0 N', 'OW1 K AH0 N', 'EH1 L AH0 S IY0', 'EH2 R', 'EH1 M AH0 D IY0', 'AW1 N T', 'IH1 K W AH0 T IY0', 'IH1 K W IH0 T IY0', 'OW1 W ER0 R', 'AO1 R N', 'AY2', 'IH1 F AE0 K', 'AW1 T', 'AE1 K', 'IH1 D', 'AH1 M P AH0 T IH0 D', 'EY1 B AH0 L D', 'AO1 R AH0 T IY0', 'IH1 S AH0 T IY0', 'IH1 T L IY0', 'AY1 AH0 T IY0', 'EY1 N Z', 'IY1 F EH0 K T', 'AA1 R G', 'AA1 T', 'AE1 NG', 'AY1 R', 'IH1 N T R AH0 S T', 'EH1 M B ER0 D', 'EY1 N JH', 'AA1 R G Y AH0 M AH0 N T', 'ER1 N AH0 T IY0', 'AY1 L', 'AE1 N S', 'IH1 G N ER0 AH0 N S', 'AO1 R D', 'OW1 T', 'AA1 T AH0 N', 'AE1 V', 'OW1 R L UH0 K', 'EH1 V IH0 S', 'AH1 S', 'EY1 B AH0 S', 'UW1 T', 'AO1 R IY0', 'ER1 D Z', 'AO1 R D Z', 'AE1 R S', 'AA1 N IH0 SH T', 'EH1 L AH0 JH AH0 N S', 'EH1 S IH0 NG', 'EH1 S T AH0 M AH0 T', 'IY1 S IH0 NG', 'AE1 N T IH0 NG', 'ER1 V IH0 NG', 'AA1 N T IH0 NG', 'OW1 IH0 NG', 'EY1 K IH0 NG', 'AE1 T ER0', 'AO1 L T', 'AE1 F T ER0 L UH0 S', 'AA1 R OW0', 'OW2', 'AO1 R S', 'AA1 S T S', 'AO1 NG Z', 'IH1 S T ER0 IY0', 'EH1 K S AH0 L AH0 N S', 'IH1 G N AH0 T IY0', 'IH1 V L AH0 JH', 'EH1 JH', 'AA1 N T AH0 N AH0 S', 'IH1 N', 'EH1 L Z', 'OW1 TH', 'EH1 N D Z', 'AY2 D', 'IH1 K S', 'IY1 M IH0 NG', 'IH1', 'AA1 L AH0 T R IY0', 'AA1 N F IH0 N', 'IH1 F ER0 AH0 N S', 'AY1 T S', 'AA1 F AH0 S IY0 Z', 'IH1 G Y ER0 IH0 NG', 'OW1 L', 'EH1 N D ER0', 'EH1 S IH0 JH', 'AY1 B Z', 'AA1 N Y UW0 M AH0 N T', 'EH1 R IH0 K T ER0', 'IH1 R AH0 T', 'EH1 JH IH0 S T ER0', 'EH1 R AH0 T', 'AA1 NG', 'UH1 D', 'AH1 B D UW0', 'IH1 NG K', 'EH1 K SH AH0 N', 'AE1 CH', 'IY1 CH ER0', 'AE1 T ER0 IY0', 'EH1 M B AH0 L', 'IY1 IH0 NG', 'AH1 P', 'IH1 R ER0', 'AE1 K S AH0 D AH0 N T S', 'EH1 N T S', 'IH1 R AH0 N IY0', 'EY1 N T IY0', 'AY1 N D Z', 'AA1 R K', 'EY1 K AH0 N', 'IY1 K S', 'IH1 N D Z', 'ER1 JH', 'IY1 T N AH0 S', 'IY1 D IH0 NG', 'EH1 R Z', 'IH1 R Z', 'IH1 T IH0 D', 'EH1 V ER0', 'IH1 T AH0 D', 'IY1 V ER0', 'EH1 T ER0', 'EY1 T ER0', 'IY1 L', 'EH1 M B ER0', 'EH1 V AH0 L', 'IH1 S T', 'AE1 DH ER0', 'AE1 K S AH0 D AH0 N T', 'AO1 L Z', 'EH1 R AH0 T IH0 K', 'IH2 K', 'AE1 N AH0 P IY0', 'AA1 N ER0 IH0 NG', 'UW1 IH0 N IH0 NG', 'EY1 V ER0', 'UH1 R', 'IH1 P S', 'OW1 S', 'AA1 R D', 'EY1 SH AH0 S', 'UH1 K S', 'AE1 N S F ER0', 'AH1 T AH0 L T IY0 Z', 'EH1 N AH0 M IY0 Z', 'IH1 N JH ER0 IY0 Z', 'AY1 T AH0 D', 'AE1 N', 'AO1 R N AH0 M AH0 N T S', 'IY1 V AH0 L', 'IY1 N D', 'EH2 S', 'IY1 Z', 'EH1 P T', 'IY1 Z AH0 N', 'EH1 R IH0 NG', 'AO1 S T', 'AY1 N D N AH0 S', 'AA1 N S T AH0 N S IY0', 'AY1 ER0 D', 'AO1 R M', 'AA1 R M', 'EH1 CH UW0 AH0 L'])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rhyming_part_to_idx.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value for one of these keys is a dictionary mapping words to lines that end with those words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'spring': [9, 876, 1359, 1420],\n",
       "             'sing': [109, 532, 1083, 1422, 1483],\n",
       "             'bring': [534],\n",
       "             'king': [874],\n",
       "             'wing': [1085],\n",
       "             'thing': [1361]})"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rhyming_part_to_idx['IH1 NG']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are this many unique line-ending rhyming parts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "388"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rhyming_part_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the couplet generation algorithm to work, we'll need to filter out any rhyming part whose dictionary contains only one value (i.e., there's only one line that ends with this rhyming part):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhyme_map = {k: v for k, v in rhyming_part_to_idx.items() if len(v) > 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we can generate couplets. To do this, we need to:\n",
    "\n",
    "* Pick a random rhyming part\n",
    "* Pick two random words that end lines having that rhyming part\n",
    "* Pick the indexes of two lines that end with those words, at random\n",
    "* Print out the resulting lines!\n",
    "\n",
    "The following cell does exactly this, in a `for` loop, to generate a little poem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Askance and strangely; but, by all above,\n",
      "Admit impediments. Love is not love\n",
      "Buy terms divine in selling hours of dross;\n",
      "And losing her, my friend hath found that loss;\n",
      "  For thy sweet love remember'd such wealth brings\n",
      "Divert strong minds to the course of altering things;\n",
      "By chance, or nature's changing course untrimm'd: \n",
      "When proud-pied April, dress'd in all his trim,\n",
      "When love, converted from the thing it was,\n",
      "  Since why to love I can allege no cause.\n",
      "Against that time do I ensconce me here,\n",
      "By unions married, do offend thine ear,\n",
      "With thy sweet fingers when thou gently sway'st\n",
      "Made more or less by thy continual haste.\n"
     ]
    }
   ],
   "source": [
    "for i in range(7):\n",
    "    # get a random rhyming part\n",
    "    rhyming_part = random.choice(list(rhyme_map.keys()))\n",
    "    # get the set of lines for two random words that end lines with this rhyme\n",
    "    a_set, b_set = random.sample(list(rhyme_map[rhyming_part].keys()), 2)\n",
    "    # randomly select a line index from the sets for both words\n",
    "    a_idx = random.choice(rhyme_map[rhyming_part][a_set])\n",
    "    b_idx = random.choice(rhyme_map[rhyming_part][b_set])\n",
    "    print(\"\\n\".join([sonnet_lines[a_idx], sonnet_lines[b_idx]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
